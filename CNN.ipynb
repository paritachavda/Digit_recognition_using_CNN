{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paritachavda/Digit_recognition_using_CNN/blob/master/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uT4pKzMVC1v4",
        "colab_type": "code",
        "outputId": "536c281e-a867-41f5-89dc-a89d7ba78e78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train.shape\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "32HEgewWC45H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "seed = 7\n",
        "np.random.seed(seed)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x7FMbINpUL0v",
        "colab_type": "code",
        "outputId": "78211171-bd7d-478a-c5f4-7f5344463d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#plot the first image in the dataset\n",
        "plt.imshow(X_train[2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd93e60f278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "uLWY2ePALCmP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z1LelPOpLbAJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = X_train / 255\n",
        "X_test = X_test / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1UIAaI9VLeHe",
        "colab_type": "code",
        "outputId": "1cfd7f97-5719-45e1-d264-86fcafe1cf78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.utils import np_utils\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "y_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "GJH-uIZzLp-m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Convolution2D:\n",
        "\n",
        "    def __init__(self, inputs_channel, num_filters, kernel_size, padding, stride, learning_rate, name):\n",
        "        # weight size: (F, C, K, K)\n",
        "        # bias size: (F) \n",
        "        self.F = num_filters\n",
        "      \n",
        "        self.K = kernel_size\n",
        "        self.C = inputs_channel\n",
        "\n",
        "        self.weights = np.zeros((self.F, self.C, self.K, self.K))\n",
        "        self.bias = np.zeros((self.F, 1))\n",
        "        for i in range(0,self.F):\n",
        "            #loc = mean scale = std dev size = no. of samples of distibution\n",
        "            self.weights[i,:,:,:] = np.random.normal(loc=0, scale=np.sqrt(1./(self.C*self.K*self.K)), size=(self.C, self.K, self.K))\n",
        "\n",
        "        self.p = padding\n",
        "        self.s = stride\n",
        "        self.lr = learning_rate\n",
        "        self.name = name\n",
        "\n",
        "    def zero_padding(self, inputs, size):\n",
        "        w, h = inputs.shape[0], inputs.shape[1]\n",
        "        new_w = 2 * size + w\n",
        "        new_h = 2 * size + h\n",
        "        out = np.zeros((new_w, new_h))\n",
        "        out[size:w+size, size:h+size] = inputs\n",
        "        return out\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        C = inputs.shape[0]\n",
        "        W = inputs.shape[1]+2*self.p\n",
        "        H = inputs.shape[2]+2*self.p\n",
        "        self.inputs = np.zeros((C, W, H))\n",
        "        for c in range(inputs.shape[0]):\n",
        "            self.inputs[c,:,:] = self.zero_padding(inputs[c,:,:], self.p)\n",
        "        WW = (W - self.K)/self.s + 1\n",
        "        HH = (H - self.K)/self.s + 1\n",
        "        feature_maps = np.zeros((self.F, int(WW), int(HH)))\n",
        "\n",
        "        for f in range(self.F):\n",
        "            for w in range(int(WW)):\n",
        "                for h in range(int(HH)):\n",
        "                    feature_maps[f,w,h]=np.sum(self.inputs[:,w:w+self.K,h:h+self.K]*self.weights[f,:,:,:])+self.bias[f]\n",
        "        return feature_maps\n",
        "\n",
        "    def backward(self, dy):\n",
        "\n",
        "        C, W, H = self.inputs.shape\n",
        "        dx = np.zeros(self.inputs.shape)\n",
        "        dw = np.zeros(self.weights.shape)\n",
        "        db = np.zeros(self.bias.shape)\n",
        "\n",
        "        F, W, H = dy.shape\n",
        "        for f in range(F):\n",
        "            for w in range(W):\n",
        "                for h in range(H):\n",
        "                    dw[f,:,:,:]+=dy[f,w,h]*self.inputs[:,w:w+self.K,h:h+self.K]\n",
        "                    dx[:,w:w+self.K,h:h+self.K]+=dy[f,w,h]*self.weights[f,:,:,:]\n",
        "\n",
        "        for f in range(F):\n",
        "            db[f] = np.sum(dy[f, :, :])\n",
        "\n",
        "        self.weights -= self.lr * dw\n",
        "        self.bias -= self.lr * db\n",
        "        return dx\n",
        "\n",
        "    def extract(self):\n",
        "        return {self.name+'.weights':self.weights, self.name+'.bias':self.bias}\n",
        "\n",
        "    def feed(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "                                \n",
        "\n",
        "class Maxpooling2D:\n",
        "\n",
        "    def __init__(self, pool_size, stride, name):\n",
        "        self.pool = pool_size\n",
        "        self.s = stride\n",
        "        self.name = name\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        C, W, H = inputs.shape\n",
        "        new_width = (W - self.pool)/self.s + 1\n",
        "        new_height = (H - self.pool)/self.s + 1\n",
        "        out = np.zeros((int(C), int(new_width),int(new_height)),dtype=int)\n",
        "        for c in range(int(C)):\n",
        "            for w in range(int(W/self.s)):\n",
        "                for h in range(int(H/self.s)):\n",
        "                    out[c, w, h] = np.max(self.inputs[c, w*self.s:w*self.s+self.pool, h*self.s:h*self.s+self.pool])\n",
        "        return out\n",
        "\n",
        "    def backward(self, dy):\n",
        "        C, W, H = self.inputs.shape\n",
        "        dx = np.zeros(self.inputs.shape)\n",
        "        \n",
        "        for c in range(C):\n",
        "            for w in range(0, W, self.pool):\n",
        "                for h in range(0, H, self.pool):\n",
        "                    st = np.argmax(self.inputs[c,w:w+self.pool,h:h+self.pool])\n",
        "                    (idx, idy) = np.unravel_index(st, (self.pool, self.pool))\n",
        "                    dx[c, int(w+idx), int(h+idy)] = dy[c, int(w/self.pool), int(h/self.pool)]\n",
        "        return dx\n",
        "\n",
        "    def extract(self):\n",
        "        return \n",
        "    \n",
        "class FullyConnected:\n",
        "\n",
        "    def __init__(self, num_inputs, num_outputs, learning_rate, name):\n",
        "        self.weights = 0.01*np.random.rand(num_inputs, num_outputs)\n",
        "        self.bias = np.zeros((num_outputs, 1))\n",
        "        self.lr = learning_rate\n",
        "        self.name = name\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        return np.dot(self.inputs, self.weights) + self.bias.T\n",
        "\n",
        "    def backward(self, dy):\n",
        "\n",
        "        if dy.shape[0] == self.inputs.shape[0]:\n",
        "            dy = dy.T\n",
        "        dw = dy.dot(self.inputs)\n",
        "        db = np.sum(dy, axis=1, keepdims=True)\n",
        "        dx = np.dot(dy.T, self.weights.T)\n",
        "\n",
        "        self.weights -= self.lr * dw.T\n",
        "        self.bias -= self.lr * db\n",
        "\n",
        "        return dx\n",
        "\n",
        "    def extract(self):\n",
        "        return {self.name+'.weights':self.weights, self.name+'.bias':self.bias}\n",
        "\n",
        "    def feed(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "class Flatten:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def forward(self, inputs):\n",
        "        self.C, self.W, self.H = inputs.shape\n",
        "        return inputs.reshape(1, self.C*self.W*self.H)\n",
        "    def backward(self, dy):\n",
        "        return dy.reshape(self.C, self.W, self.H)\n",
        "    def extract(self):\n",
        "        return\n",
        "\n",
        "class ReLu:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        ret = inputs.copy()\n",
        "        ret[ret < 0] = 0\n",
        "        return ret\n",
        "    def backward(self, dy):\n",
        "        dx = dy.copy()\n",
        "        dx[self.inputs < 0] = 0\n",
        "        return dx\n",
        "    def extract(self):\n",
        "        return\n",
        "\n",
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def forward(self, inputs):\n",
        "        exp = np.exp(inputs, dtype=np.float)\n",
        "        self.out = exp/np.sum(exp)\n",
        "        return self.out\n",
        "    def backward(self, dy):\n",
        "        return self.out.T - dy.reshape(dy.shape[0],1)\n",
        "    def extract(self):\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ud8bdJbtM1Vu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def cross_entropy(inputs, labels):\n",
        "\n",
        "    out_num = labels.shape[0]\n",
        "    \n",
        "    p = np.sum(labels.reshape(1,out_num)*inputs)\n",
        "    loss = -np.log(p)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2-wtrZmQRXM_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from time import *\n",
        "class Net:\n",
        "    def __init__(self):\n",
        "        lr = 0.01\n",
        "        self.layers = []\n",
        "        self.layers.append(Convolution2D(inputs_channel=1, num_filters=6, kernel_size=5, padding=2, stride=1, learning_rate=lr, name='conv1'))\n",
        "        self.layers.append(ReLu())\n",
        "        self.layers.append(Maxpooling2D(pool_size=2, stride=2, name='maxpool2'))\n",
        "        self.layers.append(Convolution2D(inputs_channel=6, num_filters=16, kernel_size=5, padding=0, stride=1, learning_rate=lr, name='conv3'))\n",
        "        self.layers.append(ReLu())\n",
        "        self.layers.append(Maxpooling2D(pool_size=2, stride=2, name='maxpool4'))\n",
        "        self.layers.append(Convolution2D(inputs_channel=16, num_filters=120, kernel_size=5, padding=0, stride=1, learning_rate=lr, name='conv5'))\n",
        "        self.layers.append(ReLu())\n",
        "        self.layers.append(Flatten())\n",
        "        self.layers.append(FullyConnected(num_inputs=120, num_outputs=84, learning_rate=lr, name='fc6'))\n",
        "        self.layers.append(ReLu())\n",
        "        self.layers.append(FullyConnected(num_inputs=84, num_outputs=10, learning_rate=lr, name='fc7'))\n",
        "        self.layers.append(Softmax())\n",
        "        self.lay_num = len(self.layers)\n",
        "\n",
        "    def train(self, training_data, training_label, batch_size, epoch, weights_file):\n",
        "        total_acc = 0\n",
        "        for e in range(epoch):\n",
        "            for batch_index in range(0, training_data.shape[0], batch_size):\n",
        "                # batch input\n",
        "                if batch_index + batch_size < training_data.shape[0]:\n",
        "                    data = training_data[batch_index:batch_index+batch_size]\n",
        "                    label = training_label[batch_index:batch_index + batch_size]\n",
        "                else:\n",
        "                    data = training_data[batch_index:training_data.shape[0]]\n",
        "                    label = training_label[batch_index:training_label.shape[0]]\n",
        "                loss = 0\n",
        "                acc = 0\n",
        "                start_time = time()\n",
        "                for b in range(batch_size):\n",
        "                    x = data[b]\n",
        "                    y = label[b]\n",
        "                    # forward pass\n",
        "                    for l in range(self.lay_num):\n",
        "                        output = self.layers[l].forward(x)\n",
        "                        x = output\n",
        "                    loss += cross_entropy(output, y)\n",
        "                    if np.argmax(output) == np.argmax(y):\n",
        "                        acc += 1\n",
        "                        total_acc += 1\n",
        "                    # backward pass\n",
        "                    dy = y\n",
        "                    for l in range(self.lay_num-1, -1, -1):\n",
        "                        dout = self.layers[l].backward(dy)\n",
        "                        dy = dout\n",
        "                # time\n",
        "                end_time = time()\n",
        "                batch_time = end_time-start_time\n",
        "                remain_time = (training_data.shape[0]*epoch-batch_index-training_data.shape[0]*e)/batch_size*batch_time\n",
        "                hrs = int(remain_time)/3600\n",
        "                mins = int((remain_time/60-hrs*60))\n",
        "                secs = int(remain_time-mins*60-hrs*3600)\n",
        "                # result\n",
        "                loss /= batch_size\n",
        "                batch_acc = float(acc)/float(batch_size)\n",
        "                training_acc = float(total_acc)/float((batch_index+batch_size)*(e+1))\n",
        "                print('=== Epoch: {0:d}/{1:d} === Iter:{2:d} === Loss: {3:.2f} === BAcc: {4:.2f} === TAcc: {5:.2f} === Remain: {6:d} Hrs {7:d} Mins {8:d} Secs ==='.format(e,epoch,batch_index+batch_size,loss,batch_acc,training_acc,int(hrs),int(mins),int(secs)))\n",
        "        # dump weights and bias\n",
        "        obj = []\n",
        "        for i in range(self.lay_num):\n",
        "            cache = self.layers[i].extract()\n",
        "            obj.append(cache)\n",
        "        with open(weights_file, 'wb') as handle:\n",
        "            pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "    def test(self, data, label, test_size):\n",
        "        toolbar_width = 40\n",
        "        sys.stdout.write(\"[%s]\" % (\" \" * (toolbar_width-1)))\n",
        "        sys.stdout.flush()\n",
        "        sys.stdout.write(\"\\b\" * (toolbar_width))\n",
        "        step = float(test_size)/float(toolbar_width)\n",
        "        st = 1\n",
        "        total_acc = 0\n",
        "        for i in range(test_size):\n",
        "            if i == round(step):\n",
        "                step += float(test_size)/float(toolbar_width)\n",
        "                st += 1\n",
        "                sys.stdout.write(\".\")\n",
        "                sys.stdout.flush()\n",
        "            x = data[i]\n",
        "            y = label[i]\n",
        "            for l in range(self.lay_num):\n",
        "                output = self.layers[l].forward(x)\n",
        "                x = output\n",
        "            if np.argmax(output) == np.argmax(y):\n",
        "                total_acc += 1\n",
        "        sys.stdout.write(\"\\n\")\n",
        "        print('=== Test Size:{0:d} === Test Acc:{1:.2f} ==='.format(test_size, float(total_acc)/float(test_size)))\n",
        "\n",
        "\n",
        "    def test_with_pretrained_weights(self, data, label, test_size, weights_file):\n",
        "        with open(weights_file, 'rb') as handle:\n",
        "            b = pickle.load(handle,encoding='iso-8859-1')\n",
        "        self.layers[0].feed(b[0]['conv1.weights'], b[0]['conv1.bias'])\n",
        "        self.layers[3].feed(b[3]['conv3.weights'], b[3]['conv3.bias'])\n",
        "        self.layers[6].feed(b[6]['conv5.weights'], b[6]['conv5.bias'])\n",
        "        self.layers[9].feed(b[9]['fc6.weights'], b[9]['fc6.bias'])\n",
        "        self.layers[11].feed(b[11]['fc7.weights'], b[11]['fc7.bias'])\n",
        "        toolbar_width = 40\n",
        "        sys.stdout.write(\"[%s]\" % (\" \" * (toolbar_width-1)))\n",
        "        sys.stdout.flush()\n",
        "        sys.stdout.write(\"\\b\" * (toolbar_width))\n",
        "        step = float(test_size)/float(toolbar_width)\n",
        "        st = 1\n",
        "        total_acc = 0\n",
        "        for i in range(test_size):\n",
        "            if i == round(step):\n",
        "                step += float(test_size)/float(toolbar_width)\n",
        "                st += 1\n",
        "                sys.stdout.write(\".\")\n",
        "                sys.stdout.flush()\n",
        "            x = data[i]\n",
        "            y = label[i]\n",
        "            for l in range(self.lay_num):\n",
        "                output = self.layers[l].forward(x)\n",
        "                x = output\n",
        "            if np.argmax(output) == np.argmax(y):\n",
        "                total_acc += 1\n",
        "        sys.stdout.write(\"\\n\")\n",
        "        print('=== Test Size:{0:d} === Test Acc:{1:.2f} ==='.format(test_size, float(total_acc)/float(test_size)))\n",
        "        \n",
        "    def predict_with_pretrained_weights(self, inputs, weights_file):\n",
        "        with open(weights_file, 'rb') as handle:\n",
        "            b = pickle.load(handle,encoding='iso-8859-1')\n",
        "        self.layers[0].feed(b[0]['conv1.weights'], b[0]['conv1.bias'])\n",
        "        self.layers[3].feed(b[3]['conv3.weights'], b[3]['conv3.bias'])\n",
        "        self.layers[6].feed(b[6]['conv5.weights'], b[6]['conv5.bias'])\n",
        "        self.layers[9].feed(b[9]['fc6.weights'], b[9]['fc6.bias'])\n",
        "        self.layers[11].feed(b[11]['fc7.weights'], b[11]['fc7.bias'])\n",
        "        for l in range(self.lay_num):\n",
        "            output = self.layers[l].forward(inputs)\n",
        "            inputs = output\n",
        "        digit = np.argmax(output)\n",
        "        probability = output[0, digit]\n",
        "        return digit, probability\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iHQcmB_STAA2",
        "colab_type": "code",
        "outputId": "67ca4c18-68b9-4d24-b20c-4f684aa5dfa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1054
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "print('Training Lenet......')\n",
        "net.train(X_train, y_train, 256, 5, 'weights.pkl')\n",
        "\n",
        "# net.test(testing_data, testing_labels, 100)\n",
        "# net.test_with_pretrained_weights(X_test, y_test,100, 'pretrained_weights.pkl')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Lenet......\n",
            "=== Epoch: 0/5 === Iter:256 === Loss: 2.30 === BAcc: 0.11 === TAcc: 0.11 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:512 === Loss: 2.30 === BAcc: 0.13 === TAcc: 0.12 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:768 === Loss: 2.31 === BAcc: 0.10 === TAcc: 0.12 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:1024 === Loss: 2.30 === BAcc: 0.12 === TAcc: 0.12 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:1280 === Loss: 2.30 === BAcc: 0.11 === TAcc: 0.12 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:1536 === Loss: 2.31 === BAcc: 0.12 === TAcc: 0.12 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:1792 === Loss: 2.30 === BAcc: 0.08 === TAcc: 0.11 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:2048 === Loss: 2.31 === BAcc: 0.07 === TAcc: 0.11 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:2304 === Loss: 2.30 === BAcc: 0.11 === TAcc: 0.11 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:2560 === Loss: 2.30 === BAcc: 0.12 === TAcc: 0.11 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:2816 === Loss: 2.30 === BAcc: 0.08 === TAcc: 0.11 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:3072 === Loss: 2.30 === BAcc: 0.12 === TAcc: 0.11 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:3328 === Loss: 2.31 === BAcc: 0.09 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:3584 === Loss: 2.30 === BAcc: 0.11 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:3840 === Loss: 2.31 === BAcc: 0.07 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:4096 === Loss: 2.30 === BAcc: 0.11 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:4352 === Loss: 2.30 === BAcc: 0.11 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:4608 === Loss: 2.30 === BAcc: 0.12 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:4864 === Loss: 2.31 === BAcc: 0.08 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:5120 === Loss: 2.30 === BAcc: 0.12 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:5376 === Loss: 2.30 === BAcc: 0.11 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:5632 === Loss: 2.31 === BAcc: 0.10 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:5888 === Loss: 2.30 === BAcc: 0.09 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:6144 === Loss: 2.30 === BAcc: 0.09 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:6400 === Loss: 2.31 === BAcc: 0.11 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:6656 === Loss: 2.31 === BAcc: 0.11 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:6912 === Loss: 2.31 === BAcc: 0.09 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:7168 === Loss: 2.31 === BAcc: 0.11 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:7424 === Loss: 2.31 === BAcc: 0.11 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:7680 === Loss: 2.30 === BAcc: 0.14 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:7936 === Loss: 2.30 === BAcc: 0.12 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:8192 === Loss: 2.30 === BAcc: 0.12 === TAcc: 0.11 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:8448 === Loss: 2.30 === BAcc: 0.11 === TAcc: 0.11 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:8704 === Loss: 2.30 === BAcc: 0.13 === TAcc: 0.11 === Remain: 16 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:8960 === Loss: 2.31 === BAcc: 0.09 === TAcc: 0.11 === Remain: 16 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:9216 === Loss: 2.31 === BAcc: 0.11 === TAcc: 0.11 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:9472 === Loss: 2.30 === BAcc: 0.10 === TAcc: 0.11 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:9728 === Loss: 2.30 === BAcc: 0.11 === TAcc: 0.11 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:9984 === Loss: 2.31 === BAcc: 0.09 === TAcc: 0.11 === Remain: 16 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:10240 === Loss: 2.30 === BAcc: 0.07 === TAcc: 0.10 === Remain: 16 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:10496 === Loss: 2.30 === BAcc: 0.07 === TAcc: 0.10 === Remain: 16 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:10752 === Loss: 2.31 === BAcc: 0.12 === TAcc: 0.10 === Remain: 16 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:11008 === Loss: 2.31 === BAcc: 0.09 === TAcc: 0.10 === Remain: 16 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:11264 === Loss: 2.30 === BAcc: 0.09 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:11520 === Loss: 2.31 === BAcc: 0.14 === TAcc: 0.10 === Remain: 16 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:11776 === Loss: 2.31 === BAcc: 0.11 === TAcc: 0.10 === Remain: 16 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:12032 === Loss: 2.31 === BAcc: 0.04 === TAcc: 0.10 === Remain: 16 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:12288 === Loss: 2.30 === BAcc: 0.12 === TAcc: 0.10 === Remain: 16 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:12544 === Loss: 2.30 === BAcc: 0.12 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:12800 === Loss: 2.31 === BAcc: 0.11 === TAcc: 0.10 === Remain: 16 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:13056 === Loss: 2.31 === BAcc: 0.09 === TAcc: 0.10 === Remain: 16 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:13312 === Loss: 2.30 === BAcc: 0.12 === TAcc: 0.10 === Remain: 16 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:13568 === Loss: 2.31 === BAcc: 0.07 === TAcc: 0.10 === Remain: 16 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:13824 === Loss: 2.30 === BAcc: 0.10 === TAcc: 0.10 === Remain: 16 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:14080 === Loss: 2.31 === BAcc: 0.08 === TAcc: 0.10 === Remain: 17 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:14336 === Loss: 2.31 === BAcc: 0.11 === TAcc: 0.10 === Remain: 16 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:14592 === Loss: 2.31 === BAcc: 0.10 === TAcc: 0.10 === Remain: 16 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:14848 === Loss: 2.31 === BAcc: 0.09 === TAcc: 0.10 === Remain: 16 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:15104 === Loss: 2.30 === BAcc: 0.15 === TAcc: 0.10 === Remain: 16 Hrs 0 Mins 0 Secs ===\n",
            "=== Epoch: 0/5 === Iter:15360 === Loss: 2.30 === BAcc: 0.12 === TAcc: 0.10 === Remain: 16 Hrs 0 Mins 0 Secs ===\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bKO2afx1VXdG",
        "colab_type": "code",
        "outputId": "3e5a7ef8-35cd-4b5e-d80d-1f7ab3709324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "net = Net()\n",
        "print('Testing Lenet......')\n",
        "net.test_with_pretrained_weights(X_test, y_test,10000, 'pretrained_weights.pkl')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Lenet......\n",
            "[                                       ]\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b.......................................\n",
            "=== Test Size:10000 === Test Acc:0.50 ===\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}